{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "further-shelf",
   "metadata": {},
   "source": [
    "#####  CNN을 이용하여 손글씨 숫자 인식하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clear-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed=0\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuck-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "widespread-parts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tamil-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 1000개 , 테스트 데이터 300개\n",
    "X_train = X_train[:1000,:]\n",
    "y_train = y_train[:1000]\n",
    "X_test = X_test[:300,:]\n",
    "y_test = y_test[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "seasonal-headline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 28, 28), (300, 28, 28))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stock-miniature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (300, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 2차원 데이터에 색상 차원을 추가(흑백1, 칼라3)\n",
    "X_train= X_train.reshape(X_train.shape[0],28,28,1)\n",
    "X_test= X_test.reshape(X_test.shape[0],28,28,1)\n",
    "\n",
    "# 0-255 범위의 픽셀값을 0-1 사이의 범위로 변경\n",
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255\n",
    "\n",
    "# 이진분류이므로 y를 원핫 인코딩\n",
    "y_train =pd.get_dummies(y_train)\n",
    "y_test =pd.get_dummies(y_test)\n",
    "\n",
    "y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-latino",
   "metadata": {},
   "source": [
    "#####  입력층에 cnn을 추가해서 학습모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "electronic-feeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 입력층 (CNN층)\n",
    "# filters : 필터의 수, 출력의 수 (출력되는 이미지의 개수)\n",
    "# kernel_size : 필터의 크기 (3,3) (5,5), (7,7)\n",
    "# input_shape : 입력데이터의 크기 (2차원 이상인 경우에 사용)\n",
    "# padding : 컨벌루션 연산때문에 작아지는 이미지 크기를 유지할 것인 여부\n",
    "# same : 항상 같은 크기로 이미지를 유지해준다.  - padding\n",
    "# valid : 컬벌루션으로 줄어든 상태를 그대로 유지\n",
    "model.add(Conv2D(filters = 32 ,\n",
    "                     kernel_size = (3,3) , \n",
    "                     input_shape = (28,28,1),\n",
    "                     padding = \"valid\" , \n",
    "                     activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "# 은닉층에 넣기 전에 이전 데이터들을 1차원으로 변환\n",
    "model.add(Flatten())\n",
    "\n",
    "#은닉층\n",
    "model.add(Dense(units = 128, activation = \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "# 출력층\n",
    "model.add(Dense(units = 10, activation = \"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "featured-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "powered-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 1s 606us/sample - loss: 1.8257 - acc: 0.5130 - val_loss: 1.2834 - val_acc: 0.7000\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 211us/sample - loss: 0.8773 - acc: 0.7620 - val_loss: 0.6718 - val_acc: 0.8233\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 225us/sample - loss: 0.5252 - acc: 0.8430 - val_loss: 0.4822 - val_acc: 0.8600\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 249us/sample - loss: 0.3811 - acc: 0.8850 - val_loss: 0.3733 - val_acc: 0.8833\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 239us/sample - loss: 0.2973 - acc: 0.9220 - val_loss: 0.3308 - val_acc: 0.8933\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 237us/sample - loss: 0.2607 - acc: 0.9190 - val_loss: 0.3068 - val_acc: 0.9100\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 242us/sample - loss: 0.2059 - acc: 0.9400 - val_loss: 0.2611 - val_acc: 0.9300\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 242us/sample - loss: 0.1767 - acc: 0.9480 - val_loss: 0.2500 - val_acc: 0.9333\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 235us/sample - loss: 0.1576 - acc: 0.9550 - val_loss: 0.2441 - val_acc: 0.9300\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 239us/sample - loss: 0.1213 - acc: 0.9680 - val_loss: 0.2285 - val_acc: 0.9367\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 227us/sample - loss: 0.1003 - acc: 0.9810 - val_loss: 0.2296 - val_acc: 0.9233\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 232us/sample - loss: 0.0862 - acc: 0.9810 - val_loss: 0.2144 - val_acc: 0.9367\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 215us/sample - loss: 0.0744 - acc: 0.9860 - val_loss: 0.2230 - val_acc: 0.9400\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 219us/sample - loss: 0.0665 - acc: 0.9840 - val_loss: 0.2125 - val_acc: 0.9300\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 234us/sample - loss: 0.0562 - acc: 0.9880 - val_loss: 0.2266 - val_acc: 0.9433\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 0.0509 - acc: 0.9930 - val_loss: 0.2115 - val_acc: 0.9333\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 220us/sample - loss: 0.0413 - acc: 0.9920 - val_loss: 0.2046 - val_acc: 0.9433\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 212us/sample - loss: 0.0380 - acc: 0.9950 - val_loss: 0.2152 - val_acc: 0.9267\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 214us/sample - loss: 0.0273 - acc: 0.9990 - val_loss: 0.2046 - val_acc: 0.9400\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 235us/sample - loss: 0.0240 - acc: 0.9980 - val_loss: 0.1904 - val_acc: 0.9467\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 238us/sample - loss: 0.0250 - acc: 0.9960 - val_loss: 0.1999 - val_acc: 0.9333\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 227us/sample - loss: 0.0249 - acc: 0.9950 - val_loss: 0.2067 - val_acc: 0.9367\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 229us/sample - loss: 0.0215 - acc: 0.9990 - val_loss: 0.1912 - val_acc: 0.9333\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 220us/sample - loss: 0.0217 - acc: 0.9990 - val_loss: 0.2209 - val_acc: 0.9400\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 233us/sample - loss: 0.0176 - acc: 0.9990 - val_loss: 0.1876 - val_acc: 0.9433\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 218us/sample - loss: 0.0124 - acc: 1.0000 - val_loss: 0.2021 - val_acc: 0.9333\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 204us/sample - loss: 0.0112 - acc: 0.9990 - val_loss: 0.1907 - val_acc: 0.9367\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 203us/sample - loss: 0.0086 - acc: 1.0000 - val_loss: 0.1871 - val_acc: 0.9433\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 199us/sample - loss: 0.0099 - acc: 1.0000 - val_loss: 0.2047 - val_acc: 0.9300\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 223us/sample - loss: 0.0084 - acc: 0.9990 - val_loss: 0.2013 - val_acc: 0.9333\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(X_train, y_train, epochs=30, batch_size=100,\n",
    "         validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "juvenile-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0035 - acc: 1.0000\n",
      "훈련 정확도 :  [0.0034775346480309965, 1.0]\n",
      "300/300 [==============================] - 0s 66us/sample - loss: 0.2013 - acc: 0.9333\n",
      "테스트 정확도 :  [0.20134323398272197, 0.93333334]\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 정확도 : \", model.evaluate(X_train,y_train))\n",
    "print(\"테스트 정확도 : \", model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-smoke",
   "metadata": {},
   "source": [
    "#####  시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "checked-greeting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23b9518e3c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = h.history[\"acc\"]\n",
    "val_acc = h.history[\"val_acc\"]\n",
    "\n",
    "epochs = np.arange(len(acc))\n",
    "\n",
    "plt.plot(epochs,acc,c=\"red\", label=\"Train\")\n",
    "plt.plot(epochs,val_acc,c=\"blue\",label=\"Test\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-pocket",
   "metadata": {},
   "source": [
    "#####  베스트 모델을 찾아 저장하고, 더 이상 학습이 되지 않는다면 학습을 중단하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "killing-mustang",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 669 samples, validate on 331 samples\n",
      "Epoch 1/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.6796e-04 - acc: 1.0000\n",
      "Epoch 00001: val_acc improved from -inf to 0.98792, saving model to ./model/CNN-0001-0.9879.hdf5\n",
      "669/669 [==============================] - 0s 327us/sample - loss: 1.7448e-04 - acc: 1.0000 - val_loss: 0.0515 - val_acc: 0.9879\n",
      "Epoch 2/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.2960e-04 - acc: 1.0000\n",
      "Epoch 00002: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 295us/sample - loss: 1.4977e-04 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9879\n",
      "Epoch 3/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 7.8517e-05 - acc: 1.0000\n",
      "Epoch 00003: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 325us/sample - loss: 7.6512e-05 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 0.9879\n",
      "Epoch 4/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.5419e-04 - acc: 1.0000\n",
      "Epoch 00004: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 307us/sample - loss: 1.6168e-04 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 0.9879\n",
      "Epoch 5/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 2.5087e-04 - acc: 1.0000\n",
      "Epoch 00005: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 331us/sample - loss: 2.0750e-04 - acc: 1.0000 - val_loss: 0.0534 - val_acc: 0.9879\n",
      "Epoch 6/50000\n",
      "600/669 [=========================>....] - ETA: 0s - loss: 8.0836e-05 - acc: 1.0000\n",
      "Epoch 00006: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 346us/sample - loss: 8.2200e-05 - acc: 1.0000 - val_loss: 0.0534 - val_acc: 0.9879\n",
      "Epoch 7/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.7172e-04 - acc: 1.0000\n",
      "Epoch 00007: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 309us/sample - loss: 1.4376e-04 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 0.9879\n",
      "Epoch 8/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.0653e-04 - acc: 1.0000\n",
      "Epoch 00008: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 310us/sample - loss: 1.0377e-04 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9879\n",
      "Epoch 9/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 8.9244e-05 - acc: 1.0000\n",
      "Epoch 00009: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 337us/sample - loss: 9.3422e-05 - acc: 1.0000 - val_loss: 0.0520 - val_acc: 0.9879\n",
      "Epoch 10/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 5.1447e-05 - acc: 1.000 - ETA: 0s - loss: 5.8774e-05 - acc: 1.0000\n",
      "Epoch 00010: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 420us/sample - loss: 8.1069e-05 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9879\n",
      "Epoch 11/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.7128e-04 - acc: 1.0000\n",
      "Epoch 00011: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 324us/sample - loss: 1.6729e-04 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 0.9879\n",
      "Epoch 12/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.1605e-04 - acc: 1.0000\n",
      "Epoch 00012: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 353us/sample - loss: 1.1323e-04 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 0.9879\n",
      "Epoch 13/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 7.2046e-04 - acc: 1.0000\n",
      "Epoch 00013: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 355us/sample - loss: 7.0983e-04 - acc: 1.0000 - val_loss: 0.0963 - val_acc: 0.9698\n",
      "Epoch 14/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 9.5860e-04 - acc: 1.0000\n",
      "Epoch 00014: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 358us/sample - loss: 9.7098e-04 - acc: 1.0000 - val_loss: 0.1091 - val_acc: 0.9637\n",
      "Epoch 15/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 7.6425e-04 - acc: 1.0000\n",
      "Epoch 00015: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 343us/sample - loss: 7.4914e-04 - acc: 1.0000 - val_loss: 0.0751 - val_acc: 0.9819\n",
      "Epoch 16/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 3.1282e-04 - acc: 1.0000\n",
      "Epoch 00016: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 326us/sample - loss: 3.0956e-04 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9728\n",
      "Epoch 17/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9985   \n",
      "Epoch 00017: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 335us/sample - loss: 0.0015 - acc: 0.9985 - val_loss: 0.1504 - val_acc: 0.9607\n",
      "Epoch 18/50000\n",
      "450/669 [===================>..........] - ETA: 0s - loss: 0.0324 - acc: 0.9956\n",
      "Epoch 00018: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 338us/sample - loss: 0.0225 - acc: 0.9970 - val_loss: 0.1758 - val_acc: 0.9547\n",
      "Epoch 19/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 0.0257 - acc: 0.9880\n",
      "Epoch 00019: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 304us/sample - loss: 0.0209 - acc: 0.9910 - val_loss: 0.2302 - val_acc: 0.9366\n",
      "Epoch 20/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9846\n",
      "Epoch 00020: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 322us/sample - loss: 0.0426 - acc: 0.9851 - val_loss: 0.1431 - val_acc: 0.9577\n",
      "Epoch 21/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 0.0226 - acc: 0.9920\n",
      "Epoch 00021: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 312us/sample - loss: 0.0226 - acc: 0.9925 - val_loss: 0.1794 - val_acc: 0.9396\n",
      "Epoch 22/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9938\n",
      "Epoch 00022: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 324us/sample - loss: 0.0191 - acc: 0.9940 - val_loss: 0.2185 - val_acc: 0.9305\n",
      "Epoch 23/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 0.0093 - acc: 0.9980\n",
      "Epoch 00023: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 315us/sample - loss: 0.0079 - acc: 0.9985 - val_loss: 0.1418 - val_acc: 0.9517\n",
      "Epoch 24/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9969\n",
      "Epoch 00024: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 299us/sample - loss: 0.0080 - acc: 0.9955 - val_loss: 0.1391 - val_acc: 0.9547\n",
      "Epoch 25/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 0.0056 - acc: 0.9980\n",
      "Epoch 00025: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 316us/sample - loss: 0.0102 - acc: 0.9970 - val_loss: 0.1412 - val_acc: 0.9426\n",
      "Epoch 26/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 1.0000  \n",
      "Epoch 00026: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 318us/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 0.1406 - val_acc: 0.9577\n",
      "Epoch 27/50000\n",
      "450/669 [===================>..........] - ETA: 0s - loss: 0.0023 - acc: 1.0000   \n",
      "Epoch 00027: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 320us/sample - loss: 0.0040 - acc: 0.9985 - val_loss: 0.1107 - val_acc: 0.9607\n",
      "Epoch 28/50000\n",
      "450/669 [===================>..........] - ETA: 0s - loss: 0.0026 - acc: 0.9978    \n",
      "Epoch 00028: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 309us/sample - loss: 0.0027 - acc: 0.9985 - val_loss: 0.1235 - val_acc: 0.9577\n",
      "Epoch 29/50000\n",
      "450/669 [===================>..........] - ETA: 0s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 00029: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 313us/sample - loss: 0.0029 - acc: 0.9985 - val_loss: 0.1054 - val_acc: 0.9668\n",
      "Epoch 30/50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/669 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 1.0000   \n",
      "Epoch 00030: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 312us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1168 - val_acc: 0.9577\n",
      "Epoch 31/50000\n",
      "450/669 [===================>..........] - ETA: 0s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 00031: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 325us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1006 - val_acc: 0.9607\n",
      "Epoch 32/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 5.6362e-04 - acc: 1.0000\n",
      "Epoch 00032: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 332us/sample - loss: 5.4908e-04 - acc: 1.0000 - val_loss: 0.1026 - val_acc: 0.9637\n",
      "Epoch 33/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 0.0022 - acc: 0.9980    \n",
      "Epoch 00033: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 307us/sample - loss: 0.0017 - acc: 0.9985 - val_loss: 0.1188 - val_acc: 0.9547\n",
      "Epoch 34/50000\n",
      "550/669 [=======================>......] - ETA: 0s - loss: 8.5409e-04 - acc: 1.0000\n",
      "Epoch 00034: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 295us/sample - loss: 8.8757e-04 - acc: 1.0000 - val_loss: 0.1143 - val_acc: 0.9637\n",
      "Epoch 35/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 4.2937e-04 - acc: 1.0000\n",
      "Epoch 00035: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 285us/sample - loss: 4.2780e-04 - acc: 1.0000 - val_loss: 0.1164 - val_acc: 0.9668\n",
      "Epoch 36/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 4.9606e-04 - acc: 1.0000\n",
      "Epoch 00036: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 306us/sample - loss: 4.9259e-04 - acc: 1.0000 - val_loss: 0.1176 - val_acc: 0.9698\n",
      "Epoch 37/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 0.0018 - acc: 0.9980    \n",
      "Epoch 00037: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 295us/sample - loss: 0.0014 - acc: 0.9985 - val_loss: 0.1251 - val_acc: 0.9668\n",
      "Epoch 38/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 2.0978e-04 - acc: 1.0000\n",
      "Epoch 00038: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 288us/sample - loss: 5.4758e-04 - acc: 1.0000 - val_loss: 0.1420 - val_acc: 0.9607\n",
      "Epoch 39/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 0.0026 - acc: 0.9980\n",
      "Epoch 00039: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 300us/sample - loss: 0.0021 - acc: 0.9985 - val_loss: 0.1222 - val_acc: 0.9607\n",
      "Epoch 40/50000\n",
      "550/669 [=======================>......] - ETA: 0s - loss: 0.0011 - acc: 1.0000    \n",
      "Epoch 00040: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 292us/sample - loss: 9.8916e-04 - acc: 1.0000 - val_loss: 0.1113 - val_acc: 0.9698\n",
      "Epoch 41/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 8.7253e-04 - acc: 1.0000\n",
      "Epoch 00041: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 319us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1322 - val_acc: 0.9668\n",
      "Epoch 42/50000\n",
      "550/669 [=======================>......] - ETA: 0s - loss: 6.0036e-04 - acc: 1.0000\n",
      "Epoch 00042: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 291us/sample - loss: 5.3365e-04 - acc: 1.0000 - val_loss: 0.1318 - val_acc: 0.9637\n",
      "Epoch 43/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 3.5163e-04 - acc: 1.0000\n",
      "Epoch 00043: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 302us/sample - loss: 3.8973e-04 - acc: 1.0000 - val_loss: 0.1296 - val_acc: 0.9637\n",
      "Epoch 44/50000\n",
      "550/669 [=======================>......] - ETA: 0s - loss: 2.7004e-04 - acc: 1.0000\n",
      "Epoch 00044: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 291us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1270 - val_acc: 0.9698\n",
      "Epoch 45/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 2.4310e-04 - acc: 1.0000\n",
      "Epoch 00045: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 301us/sample - loss: 3.0500e-04 - acc: 1.0000 - val_loss: 0.1354 - val_acc: 0.9637\n",
      "Epoch 46/50000\n",
      "450/669 [===================>..........] - ETA: 0s - loss: 2.4576e-04 - acc: 1.0000\n",
      "Epoch 00046: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 303us/sample - loss: 4.3524e-04 - acc: 1.0000 - val_loss: 0.1262 - val_acc: 0.9668\n",
      "Epoch 47/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 2.5772e-04 - acc: 1.0000\n",
      "Epoch 00047: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 304us/sample - loss: 2.5078e-04 - acc: 1.0000 - val_loss: 0.1228 - val_acc: 0.9698\n",
      "Epoch 48/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.3010e-04 - acc: 1.0000\n",
      "Epoch 00048: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 304us/sample - loss: 1.5636e-04 - acc: 1.0000 - val_loss: 0.1242 - val_acc: 0.9668\n",
      "Epoch 49/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.3234e-04 - acc: 1.0000\n",
      "Epoch 00049: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 290us/sample - loss: 1.4412e-04 - acc: 1.0000 - val_loss: 0.1257 - val_acc: 0.9637\n",
      "Epoch 50/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.9454e-04 - acc: 1.0000\n",
      "Epoch 00050: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 307us/sample - loss: 1.9070e-04 - acc: 1.0000 - val_loss: 0.1282 - val_acc: 0.9668\n",
      "Epoch 51/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.2178e-04 - acc: 1.0000\n",
      "Epoch 00051: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 313us/sample - loss: 1.2181e-04 - acc: 1.0000 - val_loss: 0.1291 - val_acc: 0.9637\n",
      "Epoch 52/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.3853e-04 - acc: 1.0000\n",
      "Epoch 00052: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 309us/sample - loss: 1.5179e-04 - acc: 1.0000 - val_loss: 0.1287 - val_acc: 0.9637\n",
      "Epoch 53/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.1121e-04 - acc: 1.0000\n",
      "Epoch 00053: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 304us/sample - loss: 1.0001e-04 - acc: 1.0000 - val_loss: 0.1280 - val_acc: 0.9668\n",
      "Epoch 54/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.4531e-04 - acc: 1.0000\n",
      "Epoch 00054: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 309us/sample - loss: 1.4507e-04 - acc: 1.0000 - val_loss: 0.1292 - val_acc: 0.9668\n",
      "Epoch 55/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.0011e-04 - acc: 1.0000\n",
      "Epoch 00055: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 293us/sample - loss: 1.1498e-04 - acc: 1.0000 - val_loss: 0.1309 - val_acc: 0.9668\n",
      "Epoch 56/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 6.5480e-05 - acc: 1.0000\n",
      "Epoch 00056: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 299us/sample - loss: 6.8490e-05 - acc: 1.0000 - val_loss: 0.1316 - val_acc: 0.9668\n",
      "Epoch 57/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 8.4625e-04 - acc: 1.0000\n",
      "Epoch 00057: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 305us/sample - loss: 6.5900e-04 - acc: 1.0000 - val_loss: 0.1393 - val_acc: 0.9637\n",
      "Epoch 58/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.4399e-04 - acc: 1.0000\n",
      "Epoch 00058: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 328us/sample - loss: 2.1129e-04 - acc: 1.0000 - val_loss: 0.1430 - val_acc: 0.9637\n",
      "Epoch 59/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 8.5218e-04 - acc: 1.0000\n",
      "Epoch 00059: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 336us/sample - loss: 8.2972e-04 - acc: 1.0000 - val_loss: 0.1372 - val_acc: 0.9607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.6585e-04 - acc: 1.0000\n",
      "Epoch 00060: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 288us/sample - loss: 7.0567e-04 - acc: 1.0000 - val_loss: 0.1258 - val_acc: 0.9637\n",
      "Epoch 61/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 4.9047e-04 - acc: 1.0000\n",
      "Epoch 00061: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 331us/sample - loss: 4.7706e-04 - acc: 1.0000 - val_loss: 0.1532 - val_acc: 0.9547\n",
      "Epoch 62/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 6.8548e-04 - acc: 1.0000\n",
      "Epoch 00062: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 291us/sample - loss: 5.5338e-04 - acc: 1.0000 - val_loss: 0.1586 - val_acc: 0.9517\n",
      "Epoch 63/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 0.0012 - acc: 1.0000    \n",
      "Epoch 00063: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 321us/sample - loss: 8.9477e-04 - acc: 1.0000 - val_loss: 0.1503 - val_acc: 0.9577\n",
      "Epoch 64/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 2.7272e-04 - acc: 1.0000\n",
      "Epoch 00064: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 322us/sample - loss: 2.1873e-04 - acc: 1.0000 - val_loss: 0.1494 - val_acc: 0.9607\n",
      "Epoch 65/50000\n",
      "450/669 [===================>..........] - ETA: 0s - loss: 1.9705e-04 - acc: 1.0000\n",
      "Epoch 00065: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 291us/sample - loss: 2.9755e-04 - acc: 1.0000 - val_loss: 0.1463 - val_acc: 0.9607\n",
      "Epoch 66/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.1313e-04 - acc: 1.0000\n",
      "Epoch 00066: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 288us/sample - loss: 1.0063e-04 - acc: 1.0000 - val_loss: 0.1407 - val_acc: 0.9577\n",
      "Epoch 67/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 6.1300e-05 - acc: 1.0000\n",
      "Epoch 00067: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 297us/sample - loss: 6.1464e-05 - acc: 1.0000 - val_loss: 0.1403 - val_acc: 0.9547\n",
      "Epoch 68/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.8753e-04 - acc: 1.0000\n",
      "Epoch 00068: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 300us/sample - loss: 1.6876e-04 - acc: 1.0000 - val_loss: 0.1417 - val_acc: 0.9547\n",
      "Epoch 69/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.3823e-04 - acc: 1.0000\n",
      "Epoch 00069: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 318us/sample - loss: 1.8431e-04 - acc: 1.0000 - val_loss: 0.1408 - val_acc: 0.9547\n",
      "Epoch 70/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.1260e-04 - acc: 1.0000\n",
      "Epoch 00070: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 344us/sample - loss: 1.0964e-04 - acc: 1.0000 - val_loss: 0.1384 - val_acc: 0.9607\n",
      "Epoch 71/50000\n",
      "600/669 [=========================>....] - ETA: 0s - loss: 3.8934e-04 - acc: 1.0000\n",
      "Epoch 00071: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 345us/sample - loss: 3.7316e-04 - acc: 1.0000 - val_loss: 0.1376 - val_acc: 0.9637\n",
      "Epoch 72/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 2.0412e-04 - acc: 1.0000\n",
      "Epoch 00072: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 344us/sample - loss: 2.0099e-04 - acc: 1.0000 - val_loss: 0.1390 - val_acc: 0.9668\n",
      "Epoch 73/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 5.1513e-05 - acc: 1.0000\n",
      "Epoch 00073: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 356us/sample - loss: 5.0436e-05 - acc: 1.0000 - val_loss: 0.1395 - val_acc: 0.9668\n",
      "Epoch 74/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 6.0894e-05 - acc: 1.0000\n",
      "Epoch 00074: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 341us/sample - loss: 6.3628e-05 - acc: 1.0000 - val_loss: 0.1400 - val_acc: 0.9668\n",
      "Epoch 75/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 1.2537e-04 - acc: 1.0000\n",
      "Epoch 00075: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 315us/sample - loss: 1.1592e-04 - acc: 1.0000 - val_loss: 0.1405 - val_acc: 0.9668\n",
      "Epoch 76/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 8.2387e-05 - acc: 1.0000\n",
      "Epoch 00076: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 338us/sample - loss: 8.0628e-05 - acc: 1.0000 - val_loss: 0.1405 - val_acc: 0.9668\n",
      "Epoch 77/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 9.3488e-05 - acc: 1.0000\n",
      "Epoch 00077: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 321us/sample - loss: 9.1631e-05 - acc: 1.0000 - val_loss: 0.1402 - val_acc: 0.9668\n",
      "Epoch 78/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 5.5317e-05 - acc: 1.0000\n",
      "Epoch 00078: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 314us/sample - loss: 6.1936e-05 - acc: 1.0000 - val_loss: 0.1403 - val_acc: 0.9637\n",
      "Epoch 79/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 5.7850e-05 - acc: 1.0000\n",
      "Epoch 00079: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 306us/sample - loss: 7.1802e-05 - acc: 1.0000 - val_loss: 0.1407 - val_acc: 0.9637\n",
      "Epoch 80/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 5.0622e-05 - acc: 1.0000\n",
      "Epoch 00080: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 333us/sample - loss: 5.9562e-05 - acc: 1.0000 - val_loss: 0.1408 - val_acc: 0.9668\n",
      "Epoch 81/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 4.4846e-05 - acc: 1.0000\n",
      "Epoch 00081: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 339us/sample - loss: 4.5143e-05 - acc: 1.0000 - val_loss: 0.1415 - val_acc: 0.9668\n",
      "Epoch 82/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 4.8515e-05 - acc: 1.0000\n",
      "Epoch 00082: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 322us/sample - loss: 8.1083e-05 - acc: 1.0000 - val_loss: 0.1416 - val_acc: 0.9668\n",
      "Epoch 83/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 9.3311e-05 - acc: 1.0000\n",
      "Epoch 00083: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 325us/sample - loss: 9.2646e-05 - acc: 1.0000 - val_loss: 0.1418 - val_acc: 0.9668\n",
      "Epoch 84/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 5.0972e-05 - acc: 1.0000\n",
      "Epoch 00084: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 318us/sample - loss: 4.2980e-05 - acc: 1.0000 - val_loss: 0.1426 - val_acc: 0.9668\n",
      "Epoch 85/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 5.2318e-05 - acc: 1.0000\n",
      "Epoch 00085: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 318us/sample - loss: 1.5271e-04 - acc: 1.0000 - val_loss: 0.1425 - val_acc: 0.9668\n",
      "Epoch 86/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 5.5750e-05 - acc: 1.0000\n",
      "Epoch 00086: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 344us/sample - loss: 5.4304e-05 - acc: 1.0000 - val_loss: 0.1433 - val_acc: 0.9668\n",
      "Epoch 87/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 8.9388e-05 - acc: 1.0000\n",
      "Epoch 00087: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 381us/sample - loss: 8.8365e-05 - acc: 1.0000 - val_loss: 0.1443 - val_acc: 0.9637\n",
      "Epoch 88/50000\n",
      "550/669 [=======================>......] - ETA: 0s - loss: 7.3906e-05 - acc: 1.0000\n",
      "Epoch 00088: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 411us/sample - loss: 6.3767e-05 - acc: 1.0000 - val_loss: 0.1449 - val_acc: 0.9637\n",
      "Epoch 89/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.7255e-04 - acc: 1.0000\n",
      "Epoch 00089: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 369us/sample - loss: 1.6796e-04 - acc: 1.0000 - val_loss: 0.1450 - val_acc: 0.9577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 8.1966e-05 - acc: 1.0000\n",
      "Epoch 00090: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 389us/sample - loss: 8.0860e-05 - acc: 1.0000 - val_loss: 0.1444 - val_acc: 0.9577\n",
      "Epoch 91/50000\n",
      "550/669 [=======================>......] - ETA: 0s - loss: 5.2047e-05 - acc: 1.0000\n",
      "Epoch 00091: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 385us/sample - loss: 5.8382e-05 - acc: 1.0000 - val_loss: 0.1444 - val_acc: 0.9577\n",
      "Epoch 92/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 8.8701e-05 - acc: 1.0000\n",
      "Epoch 00092: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 353us/sample - loss: 8.6305e-05 - acc: 1.0000 - val_loss: 0.1432 - val_acc: 0.9637\n",
      "Epoch 93/50000\n",
      "600/669 [=========================>....] - ETA: 0s - loss: 3.9896e-05 - acc: 1.0000\n",
      "Epoch 00093: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 376us/sample - loss: 3.8143e-05 - acc: 1.0000 - val_loss: 0.1430 - val_acc: 0.9637\n",
      "Epoch 94/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 6.4240e-05 - acc: 1.0000\n",
      "Epoch 00094: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 375us/sample - loss: 6.3530e-05 - acc: 1.0000 - val_loss: 0.1430 - val_acc: 0.9637\n",
      "Epoch 95/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 7.5890e-05 - acc: 1.0000\n",
      "Epoch 00095: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 345us/sample - loss: 7.5237e-05 - acc: 1.0000 - val_loss: 0.1433 - val_acc: 0.9637\n",
      "Epoch 96/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 7.8159e-05 - acc: 1.0000\n",
      "Epoch 00096: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 349us/sample - loss: 7.6371e-05 - acc: 1.0000 - val_loss: 0.1444 - val_acc: 0.9637\n",
      "Epoch 97/50000\n",
      "450/669 [===================>..........] - ETA: 0s - loss: 3.3665e-05 - acc: 1.0000\n",
      "Epoch 00097: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 328us/sample - loss: 3.2472e-05 - acc: 1.0000 - val_loss: 0.1454 - val_acc: 0.9637\n",
      "Epoch 98/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 5.7824e-05 - acc: 1.0000\n",
      "Epoch 00098: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 361us/sample - loss: 5.6627e-05 - acc: 1.0000 - val_loss: 0.1457 - val_acc: 0.9637\n",
      "Epoch 99/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 3.8896e-05 - acc: 1.0000\n",
      "Epoch 00099: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 329us/sample - loss: 3.8363e-05 - acc: 1.0000 - val_loss: 0.1460 - val_acc: 0.9637\n",
      "Epoch 100/50000\n",
      "650/669 [============================>.] - ETA: 0s - loss: 1.1730e-04 - acc: 1.0000\n",
      "Epoch 00100: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 344us/sample - loss: 1.1458e-04 - acc: 1.0000 - val_loss: 0.1471 - val_acc: 0.9637\n",
      "Epoch 101/50000\n",
      "500/669 [=====================>........] - ETA: 0s - loss: 6.8922e-05 - acc: 1.0000\n",
      "Epoch 00101: val_acc did not improve from 0.98792\n",
      "669/669 [==============================] - 0s 314us/sample - loss: 5.6409e-05 - acc: 1.0000 - val_loss: 0.1479 - val_acc: 0.9637\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "#모델을 저장할 폴더명\n",
    "model_folder = \"./model/\"\n",
    "# 해당 폴더가 없다면\n",
    "if not os.path.exists(model_folder):\n",
    "    os.mkdir(model_folder)\n",
    "    \n",
    "#저장할 파일명 설정\n",
    "#{epoch:04d} : 반복수를 4자리로 설정\n",
    "#{acc:.4f} : 정확도를 소수점 4자리까지 표시\n",
    "\n",
    "modelpath = model_folder + \"CNN-{epoch:04d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "#베스트를 찾아서 해당파일명으로 저장\n",
    "# save_best_only : 더 나은 결과값만 저장\n",
    "#modelcheckpoint(filepath=파일패스 , monitor=기준값,save_vest_only=True/False)\n",
    "\n",
    "# 매 에포크 마다의 훈련 손실값 (loss)\n",
    "# 매 에포크 마다의 훈련 정확도 (acc)\n",
    "# 매 에포크 마다의 검증 손실값 (val_loss)\n",
    "# 매 에포크 마다의 검증 정확도 (val_acc)\n",
    "\n",
    "mc = ModelCheckpoint(filepath=modelpath,\n",
    "                    monitor=\"val_acc\",\n",
    "                    save_best_only=True,\n",
    "                    verbose=1)\n",
    "\n",
    "# EarlyStopping(monitor=기준값 ,patience= 조금 더 기다리는 횟수)\n",
    "# patience=20 -> 학습이 더 나아지지 않더라도 20회는 더 반복해줌\n",
    "\n",
    "es = EarlyStopping(monitor= \"val_acc\",\n",
    "                  patience=100)\n",
    "#학습\n",
    "#validation_split = 0.33 -> 검증데이터를 0.33만큼 추출해서 평가\n",
    "# 0.66 학습에 사용\n",
    "# callbacks -> mc와 es 두개를 조합해서 검증을함\n",
    "history = model.fit(X_train,y_train,\n",
    "                    epochs=50000, batch_size=50,\n",
    "                   validation_split=0.33,\n",
    "                   callbacks=[mc,es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "internal-assurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23b96831400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAm9UlEQVR4nO3deXxU9b3/8deHBIIsCkLgooBgVRRbRUmp+4JVsHVvbbV6i1av1bpbt5/WSrULrUtvrVYuFVyutdYqCFbrWhFwuWERKjsoFBK2CJWAELJ9fn98E7NNkglMmJwz7+fjMY/ZTmY+Zybznu98zznfr7k7IiISD+3SXYCIiKSOQl1EJEYU6iIiMaJQFxGJEYW6iEiMZKfriXv27OkDBgxI19OLiETS7NmzP3X33MbuT1uoDxgwgFmzZqXr6UVEIsnM/tXU/ep+ERGJEYW6iEiMKNRFRGIkbX3qiZSVlVFQUEBJSUm6S2l1HTt2pG/fvrRv3z7dpYhIjLSpUC8oKKBr164MGDAAM0t3Oa3G3dm4cSMFBQUMHDgw3eWISIw02/1iZhPMbIOZzW/kfjOzh8xsuZn908yO3NliSkpK6NGjR6wDHcDM6NGjR0b8IhGR3SuZPvUngJFN3H86cGDV6Qrg0V0pKO6BXi1T1lNEdq9mu1/cfZqZDWhikbOBpzyM4fuBmXUzsz7uvjZVRYpktIoK+PRT6NgROnWCtrodprwcioog1cN5V1TAtm2wfXs4b+qUnR1eo+ZO7dsn93jbtkFlZXJ1tmsHe+yR3PN37w5duqT2daqSij71fYHVta4XVN3WINTN7ApCa57+/fun4KlTa+PGjZxyyikArFu3jqysLHJzw4Fb+fn5dOjQodG/nTVrFk899RQPPfTQbqlVYqi4GJYsgcWLa84XL4Zly6C0tGa57Oymw6NbN+jZM5x69Ki5XPu2jh13vs7Nm+vWV315+fK6dcZFsr+qW/Jldsst8Jvf7Fw9zUhFqCda44Rr5+7jgHEAeXl5bW52jh49ejB37lwARo8eTZcuXbj55pu/uL+8vJzs7MQvWV5eHnl5ebujTImqykpYvx5Wr4ZVq8Jp2bKaUFxbqx2UlQUHHACDBsE3vwn9+oXAbK7FumEDLF0aWvabNzdeS+fOsPfe4bxTp6a/JDp2hHXragJ83bqax8nOhi99CQ4+GM48E/bbL9SeSu3aNawpUb0dO4bXONHrUv81Ky1NrkXdsWN4/mRUVkJJSXKt/698JbWvUS2pCPUCoF+t632BNSl43DbhkksuYe+99+bDDz/kyCOP5Lvf/S433HAD27dvZ4899uDxxx9n0KBBTJ06lfvvv5+//e1vjB49mlWrVvHJJ5+watUqbrjhBq677rp0r4q01NatMG8efPghzJ0bPrDN/bzeY4/QsisoCKFdHeCrV4dTWVnd5+jePQTiyJEhwA8+OJzvvz808cswKWVlsGlTCPjap40bw/mmTXVDb8uW8KVTP4BKS0OdhxwCp59eU+PBB4c621J3UFYW7LVXOO1utb980igVoT4FuMbMngW+BmxOSX/6DTeED1IqDRkC//3fLf6zpUuX8uabb5KVlUVxcTHTpk0jOzubN998kzvuuIMXXnihwd8sXryYt99+my1btjBo0CCuuuoq7ZPelm3aFMJ7zpya86VLa35S9+wZujXqh11TsrKgb9/Qyj7qKPjOd8Ll/v3DqV+/8JittdG8fXvo3TucdkVFRQgsbdyPhGZD3cz+DJwE9DSzAuBuoD2Au48FXgG+ASwHtgGXtlax6XL++eeTVfWTcvPmzYwaNYply5ZhZpTVb3lV+eY3v0lOTg45OTn06tWL9evX07dv391ZtiRS3W9d3eWxYEEI8X/VGiOpf3844gj43vfC+ZFHwj77NAy18vLEXSEVFSHM+/RJfVdEOsRhHTJIMnu/XNjM/Q5cnbKKqu1Ei7q1dO7c+YvLd911FyeffDKTJk1i5cqVnHTSSQn/Jicn54vLWVlZlJeXt3aZ8bNuXd3W84oVNRsBG9sAWH2+cWPDDXmN9VsffTT86EchvI84Ivx9MrKzoWvXcBJpI9rUEaVRsHnzZvbdd18AnnjiifQWExfuoaVcu+vjww/rBvCXvgQHHhj6fefNq+kTTmaPg27dQv/viBE1fcHV/cG72m8t0sYo1Fvo1ltvZdSoUTz44IMMHz483eW0Pdu2waOPwvPPh+6J5lRWhhb4v/8drmdlhQ1yp55a0/Vx+OGJN3xVVMBnnzXcAPjppzVBfvDBkJur/mDJGOapPlAgSXl5eV5/koxFixZxyCGHpKWedIjV+m7bBmPHwq9/HXarGzYsdIUko2/fEN5HHglf/nLYg0REEjKz2e7e6P7TaqnLrtm+vSbM16+HU06B0aPhuOPSXZlIRlKoy87Zvh3GjYMxY8IGzZNPhueegxNOSHdlIhlNoS4tU1JSE+Zr18KJJ8Kzz4ZzEUk7hbokp6QEHnsMfvUrWLMmtMifeQYa2aVTRNJDoS5N27EDxo+HX/4SCgvh+OPh6adDmGuPEpE2R6Euie3YARMmhDAvKIBjj4Unn4ThwxXmIm2YQr2WXRl6F2Dq1Kl06NCBY445ptVrbTWlpTVhvno1HHMMPP542KtFYS7S5inUa2lu6N3mTJ06lS5dukQz1EtL4Ykn4Be/CKMKHnVU6EM/9VSFuUiEJDlQcOaaPXs2J554IkOHDmXEiBGsrTp0/aGHHmLw4MEcdthhXHDBBaxcuZKxY8fy29/+liFDhjB9+vQ0V56k4mL44x/hoIPghz8Mg1C9+iq89x6cdpoCXSRi2mxLvS2MvOvuXHvttUyePJnc3Fz+8pe/cOeddzJhwgTGjBnDihUryMnJ4bPPPqNbt25ceeWVLW7d73bl5TBzJrzxRjh98EG4bdiwcBDRiBEKcpEIa7Oh3hbs2LGD+fPnc+qppwJQUVFBnz59ADjssMO46KKLOOecczjnnHPSWGUSPv4YXn89hPg//hFmxDELh+XffHOY+OD44xXmIjHQZkO9LYy86+4ceuihvP/++w3ue/nll5k2bRpTpkzh3nvvZcGCBWmosBGffx5C/NVXQ5CvWBFu798fzj8/9JMPH5782CwiEhltNtTbgpycHIqKinj//fc5+uijKSsrY+nSpRxyyCGsXr2ak08+meOOO45nnnmGrVu30rVrV4qLi9NT7Pr18NJLMHlyCPIdO8I438OHw49/HIL8wAPVGo+hLVtgxozwI2zGjDD95U9+Er7DJfMo1JvQrl07nn/+ea677jo2b95MeXk5N9xwAwcddBAXX3wxmzdvxt258cYb6datG2eeeSbf/va3mTx5Mr///e85/vjjW7fAJUvgxRdDkH/wQRhbfMAAuPJKOPvsMKiWptBr1Pz58M47YV7nAQN2/fHc4d13YdGiMGlSrblVUqqkBN5/P4T4P/4B+flhs0iHDqFH7cknw+mKK+COO8K27ygpKQnD669YEU47dsC3vx0G89xdKirCNr133w2vaaTGp3P3tJyGDh3q9S1cuLDBbXHW4vWtrHR/7z33W291HzTIPeSI+5FHut9zj/u8eWGZGCoudl+7NjWPtWSJ+4UXupuFl8/M/etfd//zn923b2/5461d6z5mjPtBB9W8JQMHur/5ZmrqragIb/vPf+4+fLh7Tk54jnbt3L/2Nfc77gjPtW1bWH7VKvcf/tA9O9u9Y0f3m29237AhNbWkytq17m+/7T5hgvtdd7lffLH7sce677NPzWtY+9Sunfvpp7s//7z7jh2pr6ey0n3+fPeHHnI/5xz3bt3qPv+IEe75+al/3p0BzPImslWhnkYtWt/PPw9JBOHTeuqp7g8/HD7BMVBSEsL21VfdH300fG+df757Xp57jx41H65jjnEfP959y5aWP8eKFe6XXuqeleXeqZP77beH78Gf/cx9v/3C43fv7n7tte5z5zb9WGVl7pMnu591Vng8cD/uOPfHH3d/7TX3Aw8Mt116qfumTTvxgngImsmT3Q87rGb9Dz/c/cYb3V96yf2zz5r++48/dh81KgRily7uP/mJ+7//vXO17KqiIve//tX9qqvqtkeqA7t/f/cTT3S/5JLwfjz1lPv06e6Fhe7Ll4fa+/YNy+fmut90UwjhnVVZ6b5smfu4ce4XXODeq1fdL+TLLnP/05/cP/nE/b77av4Hzz47/M+kk0K9DUt6ff/1r9AaNwst8t34ySwrc//gA/df/tJ95Ej36693X7MmNY89e7b7eee577tvTau5+tShQwjG004Lrc4xY9x/8YuaQOjSJXzw3nuv+R8nBQUhTNq3D63cG25wX7eu7jIVFe5vvBE+4B06hOcYOtT9D3+o+3IvXep+223u//EfYZnevcMX0OLFdR9v27bwpZGVFZZ5/vnkX5fKyvDF8NWvhuc44IDQoi0qSv4xalu0yP273w2P1a1baPEXFzdcrqwsfPH94x/hi/MnP3G/6KLwZVX9Pvz61+7PPec+c6b7p582/tpv3uz+t7+F8B0ypOZ97dLF/RvfCEH5xhshsEtLk1uP8nL3V15x/9a3wnsJ4ZfKuHGJ18c9vHdz5ri/8IL7/fe7X311eP5+/Wpq6tMnrOf48SHEEykudr/3Xve99gp/893vhtc1HSIX6pUx7T6or7KyMrlQnz49NCP23DN8SppQu7U7dmwIgrffdl+5MnwgklFREVqpDz7ofsYZ7l271vzzDxoUQmqPPdxvuWXnQ+ajj0KYV7eMR41yHz3a/ckn3adNc1+9OtSRSGWl+4wZoQXcuXN4jMGDwwd2/fq6y65fH1q1OTnhx81VV4XHbs7GjeFn+OGHh8fv2NH9e99zP/74cD0ry/3MM91ffLH5QJozx/2II8LfnXtuaHk25Z13ap6nf3/3xx5LPvSaM3duaGmCe8+eoVvm8svdTznFff/9w2tUvwW9337uJ5zQ8BdT9alr1/BL4uyzw5flrbe6H3VUza+XnJzQZfTzn4cv4FSty4YN7g88EN57CL+8Ro1y//GPw//WEUc07EKBEMpDhrh/5zvhC3vx4pb1WG7a5H7nneF/r1278Jwff9z8323b5r5wofvLL4cf2NOm7eSKe/Oh3qams1uxYgVdu3alR48eWIz30nB3Nm7cyJYtWxg4cGDjC44bB9dcAwMHho2hBx9MYSEsX16zEan2ac2axudhzs4Oe0MMHFhz2n//cN65M0yfHja6vf12mOoTws4yw4eH00knQa9e4bnvuScM1Ni5M9x4I9x0U5gStDnLloVJkf7857Bjzk03hYPMEk0/mowtW+AvfwmDSH7wQVjHs86CUaPC9YceCnN5fP/78NOfhnVtCfcwB/b48WGU4V694Ac/CI/fko2P5eXw4INw992QkwP33w+XXVZ3R6T/+z+4666w41KfPnDnnXD55WH5VMvPD8/1+uvQu3fd/4nap379Gm5nLy6GlSsT//+tWBE2cg4bVvN/c/TRrTs7oXt47caPD8P6l5eHjd6NrVP37ql53qKiMNnXI4+E57zssrB/wsaNdV+P6tdq3bq6f3/jjeF/Ymc0N51dmwr1srIyCgoKKCkpSUtNu1PHjh3p27cv7RPtnVJaCtdfH47wPP10eOYZtud049prwz9vNbOwR0Cif94BA8LDVP9zffJJ3X+2oqKGT9u3bxi3a/jwMJFRv36N179wYQjov/41BPott8B110GXLg2XXbkS7r037JGRkxOWu/lm6NGjZa9ZUxYuDK/NU0+FeafN4IILQpAOGrTrj1/9MdmVtsayZWGPlKlTw+s7bhxs3Rq+cF56KRw2cPvtcNVV0KnTrtfcnPLy8EWYKu5QVhb2wkmH8nJo1y6cdpc1a8LYd+PGhXWvlpUVPj+Nfbn07r3zdTYX6m2q+0U89BlU//6+7Tb38nL/+OOan/A33uj++uthI8+u7AWwZUvoBpkyJWyUWrZs53ac+fDD0BVR/ZP+gQdq9sIoLHT/0Y+a7stOtR07Qr/rrmxEa00VFaEPeM89a/rum+rnlmhYsSJ0H771VuiXLytrveciSn3qGW/OnLAFp2NH92eecfewl0O3bqEvcMqU9JbXlA8+CDvkVG94uuSSsBrZ2e5XXplcX3YmKSwMr8tPf5q+PVIkmpoL9TbV/RIr778fjuTs0iV0ZvfvH36PVV/u27duZ+Ozz4YO25494cUXqTj8SO6+O4yEO2QIvPBC6ANv66ZNC0czvvvuzvdli0jjmut+0RGlrWHz5tChW1oaAvyVV8IkzfXl5ob7u3WDt94Kh6298AJF1osLR4SbfvADePjh1t3YlEonnBCO0kxn36pIJlOot4Zrrgnzeb77Lnzta+G2HTvCbatWhRmFVq2qORUUhA2jv/kNH8zpwPnnhw2Zjz0WtqpHjZkCXSRdFOqp9txzYX+/u++uCXQIu33sv3+jfSjuYfeom24KPTPvvRfGnBARaQnNfJRKhYVhZ9Vhw8KOxkn6/HO46CK49tow2dDs2Qp0Edk5CvVUqayESy8N3SxPP5306IgzZsDQoeEgml/8AqZMSd0BEiKSeRTqqfLww+FwwAcfDIdiNqO4GK6+Okw4VFISju67447de+CEiMRPUhFiZiPNbImZLTez2xPc393MJpnZP80s38y+nPpS27CFC+G22+CMM8Ihg814+WU49FB49NGwfXT+/HAkp4jIrmo21M0sC3gEOB0YDFxoZoPrLXYHMNfdDwO+D/wu1YW2WaWloUO8a9ewu0oTx5EXFYVFzzgjjHfy3nth2r5Eh9aLiOyMZFrqw4Dl7v6Ju5cCzwJn11tmMPAWgLsvBgaYWe+UVtpW3X13mCLlscfCgA4JuMOf/gSHHBLGShk9OgwUddRRu7VSEckAyYT6vsDqWtcLqm6rbR5wHoCZDQP2AxpMPmVmV5jZLDObVZRoRKmomTEjDNV2+eVheMAEVq0K06VdfHHoav/ww/A9oP24RaQ1JBPqifoT6o8tMAbobmZzgWuBD4HyBn/kPs7d89w9Lzc3t6W1ti3FxfCf/xmOgf/tbxvcXVYWtp0eemg4dP53vwvfAYcemoZaRSRjJHPwUQFQexDWvsCa2gu4ezFwKYCFgdBXVJ3i6/rrQzN8xow6neJLlsCECWEI2HXrYMSIMIJuKiY2FhFpTjKhPhM40MwGAoXABcD3ai9gZt2AbVV97pcD06qCPp4mToQnnggjVx19NJ9/HvrKx48PGZ+VFbpc/uu/wnmM5/sQkTam2VB393IzuwZ4DcgCJrj7AjO7sur+scAhwFNmVgEsBCI4YkmS1q6FK67Ah+aRP/Juxl8RBljcsgUOOgjGjAmjE7ZkZhwRkVRJauwXd38FeKXebWNrXX4faP6Im6hzZ+P3b+Sp4h8wvvheFhyXTadOcP75YeCt445Tq1xE0ksDerXA7N+8yTfe/B0b6M2w7vA//xNG2N1zz3RXJiISKNST9MaLn3Pe/zuaHh2KmTW9gqHDstJdkohIAxppJAnPPAPf/FYOA/0T3pu0QYEuIm2WQr0Zv/1tOLT/mMoZTLv8f9nnG0PSXZKISKPU/dKIykq4/Xa47z74Vve3eLr9D+h437x0lyUi0iSFegJlZWFvlv/9X/jRCR/x0LTTyHrm6TCXqIhIG6bul3q2boUzzwyBfu+tW3h47vFknXJy2M1FRKSNU0u9lqKicATo7Nnwxz/C5W9fCSXb4Q9/0A7oIhIJCvUqK1aEcVpWr4ZJk+Cszm/Bfz0ThlQ86KB0lycikhSFOrBsGZxwQphe9M034dihJXDYVXDAAWFrqYhIRGR8qJeWwoUXhvMZM2DwYOCe34Skf+016Ngx3SWKiCQt40P9Zz8LfegTJ1YF+vLl8Mtfhg2jp52W7vJERFoko/d+mT4dfvWrsPviuecS5p27+mrIyYEHH0x3eSIiLZaxLfXNm8PERfvvHyZ/BuC55+D11+H3v9fYuSISSRkb6tdcAwUFtSYu2rwZbrwRhg6Fq65Kd3kiIjslI0P92Wfh6adh9Gg46qiqG++6K8w/N2VKmLpIRCSCMq5PffXq0BA/6ii4886qG//5T3jkkdCfnpeX1vpERHZFRoV6ZWWYaq68PLTUs6t/pzz5ZLhyzz1prU9EZFdlVPfLAw/A1KkwYQJ86UtVN7qHQ0hPPRW6d09neSIiuyxjWuoffhi6W847Dy65pNYd8+aFMQLOPTddpYmIpExGhPr27WGii549Ydy4emNzTZwI7drBWWelrT4RkVTJiO6XW2+FRYvCUf89etS7c9KkMPBLbm5aahMRSaXYt9T//nd4+GG4/voER/0vXQrz56vrRURiI9ahXlQEl14Khx4KY8YkWGDSpHCuUBeRmIh198t118G//x2O/E842OLEifDVr0K/fru9NhGR1hDblnp5eTg49LLL4LDDEixQUAD5+WF3GBGRmIhtqC9cCNu2wbHHNrLAiy+Gc3W9iEiMxDbU8/PD+bBhjSxQPYD6oEG7rSYRkdYW21CfORO6dQsz0jXw6afwzjvqehGR2IltqOfnh22gdQ40qvbSS2EgGHW9iEjMxDLUt22Djz5qputlv/3giCN2a10iIq0tqVA3s5FmtsTMlpvZ7Qnu38vMXjKzeWa2wMwuTX2pyZs7FyoqQku9gS1bwj6O553XSDNeRCS6mg11M8sCHgFOBwYDF5rZ4HqLXQ0sdPfDgZOAB8ysQ4prTVqTG0n//ncoLVXXi4jEUjIt9WHAcnf/xN1LgWeBs+st40BXMzOgC7AJKE9ppS2Qnw99+zYyzejEidCrFxxzzG6vS0SktSUT6vsCq2tdL6i6rbaHgUOANcBHwPXuXln/gczsCjObZWazioqKdrLk5s2c2UjXS0kJvPwynHOOpqwTkVhKJtQTdTx7vesjgLnAPsAQ4GEz27PBH7mPc/c8d8/LbaVRETdtguXLG+l6efNN2LpVuzKKSGwlE+oFQO3BUfoSWuS1XQpM9GA5sAI4ODUltszMmeE8YahPmgR77QUnn7xbaxIR2V2SCfWZwIFmNrBq4+cFwJR6y6wCTgEws97AIOCTVBaarOpQHzq03h3l5TB5MpxxBnRI2zZcEZFW1ewoje5ebmbXAK8BWcAEd19gZldW3T8WuBd4wsw+InTX3Obun7Zi3Y3Kz4eDDw4N8jqmT4eNG9X1IiKxltTQu+7+CvBKvdvG1rq8Bqg/BcVu5x5CfcSIBHdOnBjG3014p4hIPMTqiNKCAli/PsGeL5WVoT995Ejo3DkttYmI7A6xCvVGDzqaNQsKC9X1IiKxF7tQb98eDj+83h0TJ0J2dthIKiISY7EK9ZkzQ6Dn5NS60T2E+sknQ/fuaatNRGR3iE2oV1SEXpYGXS8LF8KyZep6EZGMEJtQX7IkDMDYINQnTgyjMZ5df7gaEZH4iU2oVx901GDPl4kT4eijGxndS0QkXmIT6vn50LVrvSlHV6wIg6ur60VEMkSsQj0vr97gi5MmhXONnS4iGSIWob5jB8ybl6DrZfLksDvM/vunpS4Rkd0tFqE+bx6UlSXYSLp4cRMTlYqIxE8sQj3hkaSlpbBhA+xbfz4PEZH4ikWoz5wJvXuHKey+sG5dOFeoi0gGiUWo5+eHVrrVnqOpsDCc77NPWmoSEUmHyIf65s2NdJ1Xh7pa6iKSQSIf6rNnh/MGe76sqZpxTy11EckgkQ/16o2kDUK9sDBMW9ez526vSUQkXWIR6gccAHvvXe+ONWtCK71OR7uISLxFPtRnzkzQSofQUlfXi4hkmEiH+po1YQq7hMcXrVmjjaQiknEiHerVIzMmDHW11EUkA0U+1LOyYMiQencUF8PWrWqpi0jGiXSo5+fDV74CnTrVu6N6d0aFuohkmMiGuntoqTfa9QLqfhGRjBPZUF++HD77rJE9X9RSF5EMFdlQTzgyYzW11EUkQ0U61Dt1gsGDE9xZWAh77QWdO+/2ukRE0imyoT5zJhx5JGRnJ7iz+mhSEZEME8lQLyuDOXOamNSosFD96SKSkSIZ6h99FOYlbTTUdTSpiGSoSIZ69ZGkCfd8qayEtWvV/SIiGSmSoZ6fDz16wMCBCe7csAHKy9VSF5GMlFSom9lIM1tiZsvN7PYE999iZnOrTvPNrMLM6g+GmzIJp6+rpskxRCSDNRvqZpYFPAKcDgwGLjSzOjsSuvt97j7E3YcA/w94x903tUK9bN0KCxc20vUCmsZORDJaMi31YcByd//E3UuBZ4Gzm1j+QuDPqSgukTlzQrd5kxtJQaEuIhkpmVDfF1hd63pB1W0NmFknYCTwQiP3X2Fms8xsVlFRUUtrBaC0FIYObaal3q4d9O69U48vIhJlyYR6op5rb2TZM4F3G+t6cfdx7p7n7nm5ubnJ1ljH178Os2ZBr16NLLBmTQj0hEcliYjEWzKhXgD0q3W9L7CmkWUvoBW7XpKiyTFEJIMlE+ozgQPNbKCZdSAE95T6C5nZXsCJwOTUlthCOppURDJYs6Hu7uXANcBrwCLgOXdfYGZXmtmVtRY9F3jd3T9vnVKTpKNJRSSDJdXx7O6vAK/Uu21svetPAE+kqrCdUlICGzeq+0VEMlYkjyht1Nq14VwtdRHJUPEKdU2OISIZLp6hrpa6iGSoeIW6xn0RkQwXr1AvLISOHaF793RXIiKSFvEK9erdGRMO3ygiEn/xCnUdTSoiGS5+oa6NpCKSweIT6u6h+0UtdRHJYPEJ9c8+g+3b1VIXkYwWn1DX5BgiIjEKdR1NKiISo1BXS11EJEahXt1S79MnvXWIiKRRvEJ9771hjz3SXYmISNrEJ9Q1OYaISIxCXUeTiojEKNTVUhcRiUmol5fDunVqqYtIxotHqK9fD5WVaqmLSMaLR6hrcgwRESAuoa5p7EREgLiEuo4mFREB4hLqhYWQlQW5uemuREQkreIT6n36hGAXEclg8Qh1TY4hIgLEJdQ1jZ2ICBCXUNfRpCIiQBxCfdu2MJWdul9ERGIQ6tqdUUTkC9EPdU1jJyLyhaRC3cxGmtkSM1tuZrc3ssxJZjbXzBaY2TupLbMJOppUROQL2c0tYGZZwCPAqUABMNPMprj7wlrLdAP+AIx091Vm1quV6m1I3S8iIl9IpqU+DFju7p+4eynwLHB2vWW+B0x091UA7r4htWU2obAQOneGrl1321OKiLRVyYT6vsDqWtcLqm6r7SCgu5lNNbPZZvb9VBXYrOrdGc1221OKiLRVzXa/AInS0hM8zlDgFGAP4H0z+8Ddl9Z5ILMrgCsA+vfv3/JqE9E0diIiX0impV4A9Kt1vS+wJsEyr7r75+7+KTANOLz+A7n7OHfPc/e83FQNvqWjSUVEvpBMqM8EDjSzgWbWAbgAmFJvmcnA8WaWbWadgK8Bi1JbagLuGvdFRKSWZrtf3L3czK4BXgOygAnuvsDMrqy6f6y7LzKzV4F/ApXAY+4+vzULB2DjRigtVUtdRKRKMn3quPsrwCv1bhtb7/p9wH2pKy0J2p1RRKSOaB9RqqNJRUTqiHaoq6UuIlJHtEO9uqXep0966xARaSOiH+q5udChQ7orERFpE6Id6pocQ0SkjmiHuo4mFRGpI9qhrpa6iEgd0Q31sjLYsEEtdRGRWqIb6mvXhmEC1FIXEflCdENd+6iLiDQQ3VDX0aQiIg1EN9TVUhcRaSC6oV5YCO3bQ48e6a5ERKTNiHao77MPtIvuKoiIpFp0E1GTY4iINBDdUNc0diIiDUQ31HU0qYhIA9EM9S1bwkndLyIidUQz1LU7o4hIQtEMdR14JCKSULRDXS11EZE6ohnq1d0vaqmLiNQRzVAvLIQ994QuXdJdiYhImxLNUNfujCIiCUUz1DWNnYhIQtENdbXURUQaiF6oV1aGWY8U6iIiDUQv1IuKoLxc3S8iIglEL9R1NKmISKOiF+o6mlREpFHRC/Xu3eG882C//dJdiYhIm5Od7gJa7Nhjw0lERBpIqqVuZiPNbImZLTez2xPcf5KZbTazuVWnn6a+VBERaU6zLXUzywIeAU4FCoCZZjbF3RfWW3S6u5/RCjWKiEiSkmmpDwOWu/sn7l4KPAuc3bpliYjIzkgm1PcFVte6XlB1W31Hm9k8M/u7mR2a6IHM7Aozm2Vms4qKinaiXBERaUoyoW4JbvN61+cA+7n74cDvgRcTPZC7j3P3PHfPy83NbVGhIiLSvGRCvQDoV+t6X2BN7QXcvdjdt1ZdfgVob2Y9U1aliIgkJZlQnwkcaGYDzawDcAEwpfYCZvYfZmZVl4dVPe7GVBcrIiJNa3bvF3cvN7NrgNeALGCCuy8wsyur7h8LfBu4yszKge3ABe5ev4tGRERamaUre82sCPjXTv55T+DTFJbTFsRtneK2PhC/dYrb+kD81inR+uzn7o1ulExbqO8KM5vl7nnpriOV4rZOcVsfiN86xW19IH7rtDPrE72xX0REpFEKdRGRGIlqqI9LdwGtIG7rFLf1gfitU9zWB+K3Ti1en0j2qYuISGJRbamLiEgCCnURkRiJXKg3N7Z7FJnZSjP7qGos+lnprqelzGyCmW0ws/m1btvbzN4ws2VV593TWWNLNbJOo82ssNa8Ad9IZ40tYWb9zOxtM1tkZgvM7Pqq2yP5PjWxPlF+jzqaWX7VwIgLzOxnVbe36D2KVJ961djuS6k1tjtwYYKx3SPFzFYCee4eyYMmzOwEYCvwlLt/ueq23wCb3H1M1Zdvd3e/LZ11tkQj6zQa2Oru96eztp1hZn2APu4+x8y6ArOBc4BLiOD71MT6fIfovkcGdHb3rWbWHpgBXA+cRwveo6i11DW2exvk7tOATfVuPht4suryk4QPXGQ0sk6R5e5r3X1O1eUtwCLCENqRfJ+aWJ/I8mBr1dX2VSenhe9R1EI92bHdo8aB181stpldke5iUqS3u6+F8AEEeqW5nlS5xsz+WdU9E4muivrMbABwBPB/xOB9qrc+EOH3yMyyzGwusAF4w91b/B5FLdSTGds9io519yOB04Grq376S9vzKPAlYAiwFnggrdXsBDPrArwA3ODuxemuZ1clWJ9Iv0fuXuHuQwhDnA8zsy+39DGiFurNju0eRe6+pup8AzCJ0M0Udeur+j2r+z83pLmeXebu66s+dJXAH4nY+1TVT/sC8Cd3n1h1c2Tfp0TrE/X3qJq7fwZMBUbSwvcoaqHe7NjuUWNmnas29GBmnYHTgPlN/1UkTAFGVV0eBUxOYy0pUf3BqnIuEXqfqjbCjQcWufuDte6K5PvU2PpE/D3KNbNuVZf3AL4OLKaF71Gk9n4BqNpF6b+pGdv9F+mtaNeY2f6E1jmE8e2fido6mdmfgZMIw4SuB+4mTGn4HNAfWAWc7+6R2fDYyDqdRPhZ78BK4IfVfZ1tnZkdB0wHPgIqq26+g9APHbn3qYn1uZDovkeHETaEZhEa3M+5+z1m1oMWvEeRC3UREWlc1LpfRESkCQp1EZEYUaiLiMSIQl1EJEYU6iIiMaJQFxGJEYW6iEiM/H+H+XBfXLE4ywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = h.history[\"acc\"]\n",
    "val_acc = h.history[\"val_acc\"]\n",
    "\n",
    "epochs = np.arange(len(acc))\n",
    "\n",
    "plt.plot(epochs,acc,c=\"red\", label=\"Train\")\n",
    "plt.plot(epochs,val_acc,c=\"blue\",label=\"Test\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sticky-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.0490 - acc: 0.9880\n",
      "훈련 정확도 :  [0.04897059880082452, 0.988]\n",
      "300/300 [==============================] - 0s 73us/sample - loss: 0.2953 - acc: 0.9500\n",
      "테스트 정확도 :  [0.2953233150020242, 0.95]\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 정확도 : \", model.evaluate(X_train,y_train))\n",
    "print(\"테스트 정확도 : \", model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-shore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-agent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-linux",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-acrylic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-wedding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-james",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
