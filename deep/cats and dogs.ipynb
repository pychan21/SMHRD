{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 구성\n",
    "\n",
    "- 총 25000 (개 12500, 고양이 12500)개로 구성된 이미지 셋\n",
    "- 칼라이미지, 이미지의 크기는 다름\n",
    "- train, test, validataion으로 분리된 파일을 압축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "압축을 풀어서 해당 폴더에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, shutil, zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축 파일명\n",
    "local_zip = \"./data/dogs-vs-cats.zip\"\n",
    "# 압축을 풀 파일을 읽어온다\n",
    "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
    "# 압축을 푼다\n",
    "zip_ref.extractall(\"./data\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축 파일명\n",
    "local_zip = \"./data/train.zip\"\n",
    "# 압축을 풀 파일을 읽어온다\n",
    "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
    "# 압축을 푼다\n",
    "zip_ref.extractall(\"./data\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축 파일명\n",
    "local_zip = \"./data/test1.zip\"\n",
    "# 압축을 풀 파일을 읽어온다\n",
    "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
    "# 압축을 푼다\n",
    "zip_ref.extractall(\"./data\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 사용할 일부 데이터를 새로운 폴더를 생성해서 복사해보자\n",
    "\n",
    "- /data/dogs_and_cats_small : 일부 데이터가 들어갈 메인 폴더\n",
    "- /data/dogs_and_cats_small/train : 훈련 데이터가 들어갈 폴더\n",
    "- /data/dogs_and_cats_small/train/dogs : 훈련 개 데이터가 들어갈 폴더\n",
    "- /data/dogs_and_cats_small/train/cats : 훈련 고양이 데이터가 들어갈 폴더\n",
    "- /data/dogs_and_cats_small/test : 테스트 데이터가 들어갈 폴더\n",
    "- /data/dogs_and_cats_small/test/dogs : 테스트 개 데이터가 들어갈 폴더\n",
    "- /data/dogs_and_cats_small/test/cats : 테스트 고양이 데이터가 들어갈 폴더\n",
    "- /data/dogs_and_cats_small/validation : 검증 데이터가 들어갈 폴더\n",
    "- /data/dogs_and_cats_small/validation/dogs : 검증 개 데이터가 들어갈 폴더\n",
    "- /data/dogs_and_cats_small/validation/cats : 검증 고양이 데이터가 들어갈 폴더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 폴더를 생성\n",
    "# 원 훈련 데이터가 들어있는 폴더\n",
    "original_dataset_dir = \"./data/train\"\n",
    "\n",
    "# 일부 데이터가 들어갈 폴더 생성\n",
    "base_dir = \"./data/dogs_and_cats_small\"\n",
    "\n",
    "if not os.path.exists(base_dir) :\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "# ./data/dogs_and_cats_small/train\n",
    "# 이어지는 폴더를 연결해준다\n",
    "train_dir = os.path.join(base_dir, \"train\") \n",
    "\n",
    "if not os.path.exists(train_dir) :\n",
    "    os.mkdir(train_dir)\n",
    "\n",
    "# ./data/dogs_and_cats_small/train/dogs \n",
    "train_dogs_dir = os.path.join(train_dir, \"dogs\") \n",
    "\n",
    "if not os.path.exists(train_dogs_dir) :\n",
    "    os.mkdir(train_dogs_dir) \n",
    "\n",
    "# ./data/dogs_and_cats_small/train/cats     \n",
    "train_cats_dir = os.path.join(train_dir, \"cats\") \n",
    "\n",
    "if not os.path.exists(train_cats_dir) :\n",
    "    os.mkdir(train_cats_dir)      \n",
    "\n",
    "# ./data/dogs_and_cats_small/test     \n",
    "test_dir = os.path.join(base_dir, \"test\") \n",
    "\n",
    "if not os.path.exists(test_dir) :\n",
    "    os.mkdir(test_dir) \n",
    "    \n",
    "# ./data/dogs_and_cats_small/test/dogs     \n",
    "test_dogs_dir = os.path.join(test_dir, \"dogs\") \n",
    "\n",
    "if not os.path.exists(test_dogs_dir) :\n",
    "    os.mkdir(test_dogs_dir) \n",
    "    \n",
    "# ./data/dogs_and_cats_small/test/cats    \n",
    "test_cats_dir = os.path.join(test_dir, \"cats\") \n",
    "\n",
    "if not os.path.exists(test_cats_dir) :\n",
    "    os.mkdir(test_cats_dir)       \n",
    "    \n",
    "# ./data/dogs_and_cats_small/validation      \n",
    "validation_dir = os.path.join(base_dir, \"validation\") \n",
    "\n",
    "if not os.path.exists(validation_dir) :\n",
    "    os.mkdir(validation_dir)  \n",
    "    \n",
    "# ./data/dogs_and_cats_small/validation/dogs    \n",
    "validation_dogs_dir = os.path.join(validation_dir, \"dogs\") \n",
    "\n",
    "if not os.path.exists(validation_dogs_dir) :\n",
    "    os.mkdir(validation_dogs_dir) \n",
    "\n",
    "# ./data/dogs_and_cats_small/validation/cats     \n",
    "validation_cats_dir = os.path.join(validation_dir, \"cats\") \n",
    "\n",
    "if not os.path.exists(validation_cats_dir) :\n",
    "    os.mkdir(validation_cats_dir)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성된 폴더로 각각 이미지들을 복사\n",
    "- 훈련 고양이 이미지 폴더 : 1000개\n",
    "- 훈련 개 이미지 폴더 : 1000개\n",
    "- 테스트 고양이 이미지 폴더 : 500개\n",
    "- 테스트 개 이미지 폴더 : 500개\n",
    "- 검증 고양이 이미지 폴더 : 500개\n",
    "- 검증 개 이미지 폴더 : 500개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 1000개의 고양이 이미지 파일을 train_cats_dir 폴더로 복사\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1000)]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파일명\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    \n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 1000개의 개 이미지 파일을 train_dogs_dir 폴더로 복사\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1000)]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파일명\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    \n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 500개의 고양이 이미지 파일을 test_cats_dir 폴더로 복사\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1000, 1500)]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파일명\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    \n",
    "    shutil.copyfile(src, dst)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 500개의 개 이미지 파일을 test_dogs_dir 폴더로 복사\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1000, 1500)]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파일명\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    \n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 500개의 고양이 이미지 파일을 validation_cats_dir 폴더로 복사\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1500, 2000)]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파일명\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    \n",
    "    shutil.copyfile(src, dst)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음부터 500개의 개 이미지 파일을 validation_dogs_dir 폴더로 복사\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1500, 2000)]\n",
    "\n",
    "for fname in fnames:\n",
    "    # 원래 이미지가 있는 폴더와 파일명\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    # 복사할 위치의 폴더와 파일명\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    \n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 개 데이터 수 : 1000\n",
      "훈련 고양이 데이터 수 : 1000\n",
      "테스트 개 데이터 수 : 500\n",
      "테스트 고양이 데이터 수 : 500\n",
      "검증 개 데이터 수 : 500\n",
      "검증 고양이 데이터 수 : 500\n"
     ]
    }
   ],
   "source": [
    "# 복사한 파일 수 확인\n",
    "# listdir() : 해당 폴더에 있는 파일을 가져온다\n",
    "print(\"훈련 개 데이터 수 : {}\".format(len(os.listdir(train_dogs_dir))))\n",
    "print(\"훈련 고양이 데이터 수 : {}\".format(len(os.listdir(train_cats_dir))))\n",
    "print(\"테스트 개 데이터 수 : {}\".format(len(os.listdir(test_dogs_dir))))\n",
    "print(\"테스트 고양이 데이터 수 : {}\".format(len(os.listdir(test_cats_dir))))\n",
    "print(\"검증 개 데이터 수 : {}\".format(len(os.listdir(validation_dogs_dir))))\n",
    "print(\"검증 고양이 데이터 수 : {}\".format(len(os.listdir(validation_cats_dir))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 전처리\n",
    "\n",
    "- 이미지를 같은 크기로 만들어 주어야 한다\n",
    "- 0-255 범위의 픽셀값들을 0-1사이 범위의 값으로 변환 -> 분산 감소\n",
    "- 라벨링\n",
    "\n",
    "- ImageDataGenerator() 함수를 사용해서 처리 \n",
    "  - 이미지 전처리\n",
    "  - 증식\n",
    "  - 이미지를 일부 개수만큼만 반복해서 처리 (batch 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 픽셀값을 0-1 사이로 변환\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# flow_from_directory() : 디렉토리로부터 이미지를 가져옴\n",
    "# flow_from_directory(폴더명, 이미지크기, 한번에 변환할 이미지수, 라벨링 모드)\n",
    "train_generator = train_gen.flow_from_directory(train_dir,\n",
    "                                               target_size=(150, 150),\n",
    "                                               batch_size=20,\n",
    "                                               # 다중분류 : categorical \n",
    "                                               # 라벨 번호는 0부터 시작\n",
    "                                               # 폴더명의 알파벳 순으로 할당\n",
    "                                               class_mode=\"binary\")\n",
    "test_generator = test_gen.flow_from_directory(test_dir,\n",
    "                                               target_size=(150, 150),\n",
    "                                               batch_size=20,\n",
    "                                               # 다중분류 : categorical \n",
    "                                               # 라벨 번호는 0부터 시작\n",
    "                                               # 폴더명의 알파벳 순으로 할당\n",
    "                                               class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n",
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "# 라벨링 결과 확인\n",
    "print(train_generator.class_indices)\n",
    "print(test_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기화를 위한 seed 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN을 입력층으로 한 신경망 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 180000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               92160512  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 92,161,921\n",
      "Trainable params: 92,161,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "# 입력층 (CNN 층)\n",
    "model1.add(Conv2D(filters=32,\n",
    "                  kernel_size=(3, 3),                  \n",
    "                  input_shape=(150, 150, 3),\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "# 은닉층\n",
    "model1.add(Dense(units=512, activation=\"relu\"))\n",
    "\n",
    "# 출력층\n",
    "model1.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-11329d2384bc>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 100 steps, validate for 50 steps\n",
      "Epoch 1/10\n",
      "  1/100 [..............................] - ETA: 3:20"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-18-11329d2384bc>:9) ]] [Op:__inference_distributed_function_841]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-11329d2384bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                          \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                          \u001b[1;31m# 전체 데이터(1000) / batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                          validation_steps=50)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-18-11329d2384bc>:9) ]] [Op:__inference_distributed_function_841]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "h = model1.fit_generator(generator=train_generator,\n",
    "                         # batch_size를 20으로 설정되어 있음\n",
    "                         # 전체 데이터(2000)를 다 읽어오려면 몇 번 \n",
    "                         # 돌아가야하는지 설정\n",
    "                         steps_per_epoch=100,\n",
    "                         epochs=10,\n",
    "                         validation_data=test_generator,\n",
    "                         # 전체 데이터(1000) / batch_size\n",
    "                         validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = h.history[\"acc\"]\n",
    "val_acc = h.history[\"val_acc\"]\n",
    "\n",
    "epoch = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epoch, acc, c=\"red\", label=\"Train acc\")\n",
    "plt.plot(epoch, val_acc, c=\"blue\", label=\"Test acc\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 증식을 이용해서 적은 데이터 개수를 늘려서 과대적합을 피해보자\n",
    "\n",
    "- data augmentation : 이미지 데이터를 회전, 이동, 확대/축소, 뒤집기 등을 이용해서 데이터를 늘리는 작업\n",
    "\n",
    "- ImageDataGenerator() 함수를 이용해서 수행\n",
    "- 증식은 훈련데이터만 해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증식 설정\n",
    "train_dataGen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=20,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.1,\n",
    "                            zoom_range=0.1,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode=\"nearest\")\n",
    "\n",
    "test_dataGen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_dataGen.flow_from_directory(train_dir,\n",
    "                                                   target_size=(150,150),\n",
    "                                                   batch_size=20,\n",
    "                                                   class_mode=\"binary\")\n",
    "test_generator = test_dataGen.flow_from_directory(test_dir,\n",
    "                                                   target_size=(150,150),\n",
    "                                                   batch_size=20,\n",
    "                                                   class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "# 입력층 (CNN 층)\n",
    "model2.add(Conv2D(filters=32,\n",
    "                  kernel_size=(3, 3),                  \n",
    "                  input_shape=(150, 150, 3),\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "# 은닉층\n",
    "model2.add(Dense(units=512, activation=\"relu\"))\n",
    "\n",
    "# 출력층\n",
    "model2.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model2.fit_generator(generator=train_generator,\n",
    "                         # batch_size를 20으로 설정되어 있음\n",
    "                         # 전체 데이터(2000)를 다 읽어오려면 몇 번 \n",
    "                         # 돌아가야하는지 설정\n",
    "                         steps_per_epoch=100,\n",
    "                         epochs=50,\n",
    "                         validation_data=test_generator,\n",
    "                         # 전체 데이터(1000) / batch_size\n",
    "                         validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = h.history[\"acc\"]\n",
    "val_acc = h.history[\"val_acc\"]\n",
    "\n",
    "epoch = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epoch, acc, c=\"red\", label=\"Train acc\")\n",
    "plt.plot(epoch, val_acc, c=\"blue\", label=\"Test acc\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전이학습 (transfer learning) : 기존의 잘 만들어진 모델을 가져다가 쓰는 것\n",
    "\n",
    "- 특성추출 : 기존의 모델을 특성추출기로만 사용\n",
    "- 미세조정 : 기존의 모델의 끝 층 (dense층에 가까운 층) 까지 파라미터를 업데이트하도록 하는 것 (우리 모델과 기존 모델의 유사성을 높이는 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 모델을 전이학습\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# weights : imagenet에 있는 가중치를 사용하겠다\n",
    "# include_top : 분류기 (Desne 층)도 사용할 것인지 여부 -> 분류기는 우리 것 사용\n",
    "# input_shape : 우리 모델의 입력 데이터 크기\n",
    "conv_base = VGG16(weights=\"imagenet\",\n",
    "                 include_top=False,\n",
    "                 input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16와 우리의 분류기 모델을 연동해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = \"./data/dogs_and_cats_small\"\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "\n",
    "dataGen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "# VGG16 특성추출기로 데이터를 보내서 특성을 추출하는 함수\n",
    "# (데이터 폴더의 경로, 데이터의 개수)\n",
    "def extract_features(directory, sample_count) :\n",
    "    # VGG16에 데이터를 보내서 받은 특성과 라벨을 저장하기 위한 변수 설정\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    # 제너레이터에서 생성된 라벨값을 저장\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    \n",
    "    # VGG16으로 넘기기 위한 데이터를 제너레이터로 생성\n",
    "    generator = dataGen.flow_from_directory(directory,\n",
    "                                           target_size=(150, 150),\n",
    "                                           batch_size=batch_size,\n",
    "                                           class_mode=\"binary\")\n",
    "    \n",
    "    i = 0   # VGG16를 호출한 횟수\n",
    "    # 제너레이터로부터 bath_size 개수만큼 데이터와 라벨을 가져온다\n",
    "    for inputs_bacth, labels_batch in generator :\n",
    "        # VGG16으로 데이터를 보내서 특성맵을 받아온다    \n",
    "        features_batch = conv_base.predict(inputs_bacth)\n",
    "        # features 리스트에 batch_size 개수만큼씩 VGG16에서 넘어온 특성을 추가\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        # labels에 batch_size 개수만큼씩 제너레이터에서 넘어온 라벨을 추가\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "        # 처리한 데이터 갯수가 전체 데이터 갯수(sample_count)보다 크면\n",
    "        if i * batch_size >= sample_count :\n",
    "            break\n",
    "            \n",
    "    return features, labels                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련, 테스트, 검증 데이터의 특성을 추출\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 만든 분류기에 VGG16에서 추출된 특성을 넣어주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성맵 데이터를 1차원으로 변환\n",
    "train_features = np.reshape(train_features, (2000, 4*4*512))\n",
    "test_features = np.reshape(test_features, (1000, 4*4*512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4*4*512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 설계한 신경망 층에 특성맵을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model3 = Sequential()\n",
    "# 은닉층\n",
    "model3.add(Dense(units=512, \n",
    "                 input_dim=4*4*512,\n",
    "                 activation=\"relu\"))\n",
    "model3.add(Dropout(0.5))\n",
    "\n",
    "# 출력층\n",
    "model3.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = model3.fit(train_features, train_labels,\n",
    "                batch_size=20,\n",
    "                epochs=30,\n",
    "                validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = h3.history[\"acc\"]\n",
    "val_acc = h3.history[\"val_acc\"]\n",
    "\n",
    "epoch = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epoch, acc, c=\"red\", label=\"Train acc\")\n",
    "plt.plot(epoch, val_acc, c=\"blue\", label=\"Test acc\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "증식을 사용한 특성추출 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "# 우리가 만든 모델에 VGG16을 끼워넣기\n",
    "model4.add(conv_base)\n",
    "\n",
    "model4.add(Flatten())\n",
    "\n",
    "model4.add(Dense(512, activation=\"relu\"))\n",
    "model4.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model4.summary()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동결 : 기존의 모델을 우리 모델에 그래도 삽입을 하면 오차역전파시에 기존 모델의 파라미터값도 갱신이 되어 많은 데이터로 학습된 기존 모델의 장점이 사라져버림 -> 기존 모델의 파라미터가 갱신이 되지 않도록 막는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동결되기 전의 훈련되는 VGG16의 가중치의 수\n",
    "print(len(model4.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16의 전체 층에 대해 동결\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동결 후의 훈련되는 VGG16의 가중치의 수\n",
    "print(len(model4.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 증식 설정\n",
    "train_dataGen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=20,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.1,\n",
    "                            zoom_range=0.1,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode=\"nearest\")\n",
    "\n",
    "test_dataGen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_dataGen.flow_from_directory(train_dir,\n",
    "                                                   target_size=(150,150),\n",
    "                                                   batch_size=20,\n",
    "                                                   class_mode=\"binary\")\n",
    "test_generator = test_dataGen.flow_from_directory(test_dir,\n",
    "                                                   target_size=(150,150),\n",
    "                                                   batch_size=20,\n",
    "                                                   class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = model4.fit_generator(generator=train_generator,\n",
    "                         steps_per_epoch=100,\n",
    "                         epochs=20,\n",
    "                         validation_data=test_generator,\n",
    "                         validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = h4.history[\"acc\"]\n",
    "val_acc = h4.history[\"val_acc\"]\n",
    "\n",
    "epoch = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epoch, acc, c=\"red\", label=\"Train acc\")\n",
    "plt.plot(epoch, val_acc, c=\"blue\", label=\"Test acc\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미세조정\n",
    "\n",
    "- 기존의 모델과 우리 모델이 잘 연결되도록 기존 모델의 아래층까지 학습이 가능하도록 만들어 주는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "# 우리가 만든 모델에 VGG16을 끼워넣기\n",
    "model4.add(conv_base)\n",
    "\n",
    "model4.add(Flatten())\n",
    "\n",
    "model4.add(Dense(512, activation=\"relu\"))\n",
    "model4.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model4.summary()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block5_conv1 층까지만 학습이 되도록 미세 조정\n",
    "# 학습이 되는 층 : block5_conv1, block5_conv2. block5_conv3, block5_conv4\n",
    "#                 block5_pool  \n",
    "\n",
    "# VGG16 모델 전체가 학습이 되도록 설정\n",
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "\n",
    "# VGG16의 신경망층을 한 층을 가져온다\n",
    "for layer in conv_base.layers:\n",
    "    # 가져온 층의 이름 block5_conv1이라면\n",
    "    if layer.name == \"block5_conv1\":\n",
    "        set_trainable = True\n",
    "        \n",
    "    if set_trainable == True :\n",
    "        layer.trainable = True\n",
    "    else :\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=\"adam\",\n",
    "               metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = model4.fit_generator(generator=train_generator,\n",
    "                         steps_per_epoch=100,\n",
    "                         epochs=20,\n",
    "                         validation_data=test_generator,\n",
    "                         validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = h4.history[\"acc\"]\n",
    "val_acc = h4.history[\"val_acc\"]\n",
    "\n",
    "epoch = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epoch, acc, c=\"red\", label=\"Train acc\")\n",
    "plt.plot(epoch, val_acc, c=\"blue\", label=\"Test acc\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
